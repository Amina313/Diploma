# -*- coding: utf-8 -*-
"""ml.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JOKbonO5CUrsTom07hbZ7bI4EDvT6w_R
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from xgboost import XGBClassifier
from lightfm import LightFM
from lightfm.evaluation import precision_at_k
from scipy.sparse import coo_matrix
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import LabelEncoder

def load_data(events_path, category_tree_path, item_properties_paths):
    """Загружает данные из CSV файлов."""
    try:
        events = pd.read_csv(events_path)
        category_tree = pd.read_csv(category_tree_path, sep=',')
        item_properties = pd.concat([pd.read_csv(path) for path in item_properties_paths])
    except FileNotFoundError as e:
        print(f"Ошибка при загрузке файлов: {e}")
        exit()

    return events, category_tree, item_properties


def preprocess_data(events_df, min_visitor_events=5, min_item_purchases=5):
    """Предобрабатывает данные."""
    events_df.drop_duplicates(inplace=True)
    events_df.fillna(0, inplace=True)

    # Удаление малоактивных пользователей
    visitor_counts = events_df['visitorid'].value_counts()
    active_visitors = visitor_counts[visitor_counts >= min_visitor_events].index
    events_df = events_df[events_df['visitorid'].isin(active_visitors)]

    # Удаление редко покупаемых товаров
    purchase_events = events_df[events_df['event'] == 'transaction']
    if not purchase_events.empty:
        item_counts = purchase_events['itemid'].value_counts()
        popular_items = item_counts[item_counts >= min_item_purchases].index
        events_df = events_df[events_df['itemid'].isin(popular_items)]

    return events_df

def plot_events_over_time(events):
    events['date'] = pd.to_datetime(events['timestamp'], unit='ms')
    events_per_day = events.groupby('date').size()
    # Анализ по часам
    events_per_hour = events.groupby(events['date'].dt.hour).size()

def create_item_factors(events):
    """Генерирует факторы для товаров."""
    item_event_counts = events['itemid'].value_counts().reset_index()
    item_event_counts.columns = ['itemid', 'event_count']
    item_purchase_counts = events[events['event'] == 'transaction']['itemid'].value_counts().reset_index()
    item_purchase_counts.columns = ['itemid', 'purchase_count']

    item_factors = pd.merge(item_event_counts, item_purchase_counts, on='itemid', how='left').fillna(0)
    item_factors['purchase_ratio'] = item_factors['purchase_count'] / item_factors['event_count']

    return item_factors

def merge_item_categories(item_factors, item_properties):
    """Добавляет категории к товарам."""
    item_categories = item_properties[item_properties['property'] == 'categoryid'][['itemid', 'value']]
    item_categories = item_categories.groupby('itemid')['value'].agg(lambda x: x.value_counts().index[0]).reset_index()
    item_categories.columns = ['itemid', 'categoryid']

    item_factors = pd.merge(item_factors, item_categories, on='itemid', how='left')
    return item_factors

def create_user_item_factors(events):
    """Генерирует факторы взаимодействий между пользователями и товарами."""
    user_item_interactions = events.groupby(['visitorid', 'itemid'])['event'].count().reset_index()
    user_item_interactions.columns = ['visitorid', 'itemid', 'interaction_count']
    user_item_purchases = events[events['event'] == 'transaction'].groupby(['visitorid', 'itemid'])['transactionid'].count().reset_index()
    user_item_purchases.columns = ['visitorid', 'itemid', 'purchase_count']

    user_item_factors = pd.merge(user_item_interactions, user_item_purchases, on=['visitorid', 'itemid'], how='left').fillna(0)
    user_item_factors['purchase_ratio'] = user_item_factors['purchase_count'] / user_item_factors['interaction_count']

    return user_item_factors

def split_data_by_time(events, train_ratio, val_ratio):
    """Разделяет данные на тренировочную, валидационную и тестовую выборки по времени."""
    events = events.sort_values('timestamp')
    n = len(events)
    train_size = int(n * train_ratio)
    val_size = int(n * val_ratio)

    train_data = events[:train_size]
    val_data = events[train_size:train_size + val_size]
    test_data = events[train_size + val_size:]

    return train_data, val_data, test_data

def validate_time_split(train_df, val_df, test_df):
    """Валидация разбиения данных по времени."""
    if not train_df['timestamp'].is_monotonic_increasing:
        raise ValueError("Данные в тренировочной выборке не отсортированы по времени.")
    if not val_df['timestamp'].is_monotonic_increasing:
        raise ValueError("Данные в валидационной выборке не отсортированы по времени.")
    if not test_df['timestamp'].is_monotonic_increasing:
        raise ValueError("Данные в тестовой выборке не отсортированы по времени.")

def create_and_train_lightfm(user_item_matrix):
    """Создает и обучает модель LightFM."""
    sparse_matrix = coo_matrix(user_item_matrix.values)
    lfm_model = LightFM(loss='warp')
    lfm_model.fit(sparse_matrix, epochs=30, num_threads=4)
    return lfm_model


def train_xgboost(X, y):
    """Обучает модель XGBoost."""
    xgb_model = XGBClassifier()
    xgb_model.fit(X, y)
    return xgb_model

def main():
    # Загрузка данных
    events, category_tree, item_properties = load_data(
        '/content/events.csv',
        '/content/category_tree.csv',
        ['/content/item_properties_part1.csv', '/content/item_properties_part2.csv']
    )

    # Предобработка
    events = preprocess_data(events)

    # Визуализация
    plot_events_over_time(events)

    # Создание факторов
    item_factors = create_item_factors(events)
    item_factors = merge_item_categories(item_factors, item_properties)
    user_item_factors = create_user_item_factors(events)

    # Разбиение данных
    train_data, val_data, test_data = split_data_by_time(events, 0.6, 0.2)
    validate_time_split(train_data, val_data, test_data)

    # Создание матрицы взаимодействия
    user_item_matrix = events.pivot_table(values='timestamp', index='visitorid', columns='itemid', aggfunc='count', fill_value=0)

    # Обучение модели LightFM
    lfm_model = create_and_train_lightfm(user_item_matrix)

    # Подготовка данных для XGBoost
    X = user_item_matrix.values[:, :-1]  # все колонки, кроме последней
    y = user_item_matrix.values[:, -1]  # последняя колонка

    # Разделение на обучающую и тестовую выборки
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Останавливаемся только на общих классах, представленных в обеих частях
    common_classes = np.intersect1d(y_train, y_test)

    # Исключаем строки с редкими классами
    mask_train = np.isin(y_train, common_classes)
    mask_test = np.isin(y_test, common_classes)

    X_train_common = X_train[mask_train]
    y_train_common = y_train[mask_train]
    X_test_common = X_test[mask_test]
    y_test_common = y_test[mask_test]

    # Повторяем процедуру перекодирования
    label_encoder = LabelEncoder()
    y_train_encoded = label_encoder.fit_transform(y_train_common)
    y_test_encoded = label_encoder.transform(y_test_common)

    # Обучение модели XGBoost
    xgb_model = train_xgboost(X_train_common, y_train_encoded)

    # Сохранение модели
    xgb_model.save_model("xgb_model.json")

if __name__ == "__main__":
    main()